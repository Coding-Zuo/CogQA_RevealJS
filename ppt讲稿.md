# ppt讲稿

1今天我要讲的论文是ACL2019上一篇叫基于大规模多跳阅读理解的认知图。那么什么是认知图谱什么是多跳阅读理解呢？我分开来解释一下

---

2认知图谱我们可能不太了解，但知识图谱我们都知道，是由谷歌提出可以构建十几亿节点的知识库。节点表示实体，边表示实体间关系，像这个人和美国是公民的关系，知识图谱。他也包含了很多技术像知识嵌入、图谱构建，知识推理啊等等。但他也有他的一些难点，如歧义问题、关系的冗余与组合爆炸等。想要真正解决这些问题可能要重新考虑知识的表示框架和方法论。因为节点只表示的是实体的描述，没有抽象的表征，没抽象的特征就不可能有认知的模型。没有更丰富的认知模型，就不可能有鲁棒性。拥有的只是大规模的数据，以此出现的新事物就不会有很大的出入。

---

3 那么基于此17年哈工大提出过事理图谱，这张图是事理图谱和知识图谱融合的示例，左边是知识图谱表示的是公司之间投资的实体关系，右边是基于<事件，论元集合，逻辑关系>的事理图谱 。其实他和认知图谱就很像了，但其主要侧重点还是在事件抽取方面的工作而不是基于图的推理， 后面我列了个表总结了一下三者的关系。先看一下什么是认知图谱

---

4我的理解是动态构建带有上下文语义信息的知识图谱并进行解释性推理，增加了新的信息粒度和信息结构，会带来更大的想象空间

他概念等于知识图谱+认知推理+逻辑表达，而不是完全基于知识图谱。

---

5这张图是唐杰教授画的，该有的技术都被涵盖了，知识图谱方面的插入构建存储、推理模型的历史方法、表达模型的方法。

---

6这张图是我总结的区别

|          | 知识图谱                                | 事理图谱                               | 认知图谱                                                    |
| :------- | :-------------------------------------- | :------------------------------------- | :---------------------------------------------------------- |
| 描述知识 | 万物本体                                | 逻辑社会                               | 认知抽象语义总结                                            |
| 研究对象 | 名词性实体及属性关系                    | 谓词性事件及其内外(空间时间域)联系     | 带有较完整的上下文的语义信息认知节点concept、融合了大量信息 |
| 构建目标 | 万物互联                                | 全逻辑库、逻辑演化模型                 | 记忆认知语义库                                              |
| 回答问题 | who/where/when/what                     | why/how                                | all                                                         |
| 组织形式 | 有向图                                  | 有向图                                 | 可以有向可以无向                                            |
| 知识形式 | <实体，属性，属性值> <实体，关系，实体> | <事件，论元集合，逻辑关系>多元组       | <实体或语义，关系(是，组成)，实体或语义>                    |
| 知识确定 | 事实是确定的                            | 逻辑不确定，有转移概率                 | 基本确定，有推理错误概率，允许有遗忘                        |
| 知识状态 | 相对静态，变化缓慢                      | 动态的                                 | 动态的                                                      |
| 知识敏感 | 精确性要求高，实时性要求高              | 可一定容错，参考逻辑                   | 可一定容错                                                  |
| 构建难点 | 知识本体的搭建、知识抽取与融合          | 事件的表示，事件的抽取；与知识图谱融合 | 语义信息提取，推理引擎赋能、受检索信息提供的限制            |

----

7认知图谱的核心理论是来自认知科学中的双过程理论。是模仿人类认知的一个过程也是  无意识的直觉系统+有意识的推理系统  概况成 隐式提取+显式推理

----

8 认知图谱在工业上是有落地的，阿里搜索事业部有认知图谱团队，他们是从电商角度构建图谱的，通过知识图谱总结出一系列Concept节点，为的是利用知识图谱提取出用户需求的描述，来做更解释性的更丰富的商品推荐。这个地方是认知图谱。

---

9什么是多跳阅读理解

先了解单段落的简单阅读理解，顾名思义，只要匹配上单个段落中的答案就可以。比如让机器做个完形填空或者单项多项选择，答案固定，已经超越人类水平。以前我们的问答系统回答的一看就知道是机器人回答的，千篇一律的答案，这机器与人之间存在的理解鸿沟。局限有以下三点  1 2 3   主要是人为选出一系列可能的答案。

-----

10那么什么是多跳阅读 

总的来说就是我从当前问题中提取出来的实体进行信息检索，找出一些段落，我再在这些段落里找有用的信息再去检索，这是一个迭代的过程。

以本文为例，问题是 有个叫2003这个电影有个场景是在洛杉矶这个咖啡馆拍的，这个电影的导演是谁？那么针对这个问题的实体我可以检索出多个段落的文本，里面可能蕴含这答案或者线索。比如第一跳我查出来有old school和 Gone in 60 seconds这个电影相关的文字，我进一步在通过这两个线索在去检索出来进一步的一些段落，最终找到了两个可能的答案。这种阅读理解更贴近人类真实了解问题的办法。对复杂问题也有很好的解答能力。

---

11多跳阅读理解的挑战主要有三点。123

-----

12看一下本文的贡献，在多跳数据集榜单上持续3个月第一名

----

13接着看一下算法流程，算法框架分为系统一，隐式提取系统和系统二显示推理系统。提出一个问题，使用系统1从检索器查到的多个文本进行提取答案和实体，构建出认知图给系统二去推理，系统二还可以通过推理的结果反向指导系统1 的线索提取。///////首先初始用问题提到的节点作为前沿节点进行初始化，算法是一个迭代的过程，收集提及前沿节点的线索，由系统一生成语义向量，遍历前沿节点，如果这是一个hop节点，用系统1在段落中找到下一跳hop或者答案候选。先遍历这些hop段中的内容y，如果y在语料库中但不在认知图中，用y创建一个新的hop节点，如果y在图中，但当前的图中节点和他没有边，给y添加一个边并让他也成为前沿节点。再遍历可能的答案，给他们和当前前沿节点连上边。再用GNN更新隐层表示，循环这个过程 直到认知图中没有前沿节点，或者图足够大了。最后用全连接层预测出概率最大的答案节点。

------

13那么我们看看作者针对这些问题的贡献。1 2

---

先整体看一下作者的设计，

提出一个问题，交给系统一去多跳的提取候选答案和hop实体，构建一个认知图交给系统2去推理，系统二更新节点隐层向量表示后还能反向指导系统一更好的进行信息提取。最终由系统二给出最终答案。这个算法流程一会再看，先介绍一下系统一和系统二。

----

系统一是 1 2 

---

14系统一采用预训练模型bert，bert是以字为编码的，输入是字的embeding加上句子的embedding，加上位置编码，这里的位置编码和transformer先验三角函数公式不一样，直接初始化为1234，句子编码为第一个句子是0第二个句子是1。每个句子是SEP分割的。

----

15作者的输入的是 问题+ 线索+段落的形式，每个训练数据的开头是CLS标签，我们以CLS提取的向量表示作为整个句子的语义向量。交给GNN去处理这个后面再说。

然后我们在分别初始化hop start和hop end 、answer start和answer end 权重矩阵，让他们分别和段落输出相乘做softmax得到每个字的概率，分别取hop和answer各自最大的概率token作为开始和结束。他的loss设计是用onehot标记答案位置，和概率做交叉熵。如果某个输出是end的概率大于start的概率，表示模型也不清楚答案是哪个就没有答案或者hop。然后系统一把得到的hop和answer当做实体放到认知图里。

-----

16再看系统二  1 2 3 

------

17系统二的图神经网络是可以更换的，作者只是使用了普通的基于有向图的GCN

右乘度归一化的度矩阵，传递指向他的节点的信息，第二个公式进行节点更新。

用现有的信息和提取的语义信息进行Linear、softmax操作，这里有一个loss的输出，可以理解为反向指导系统一，和Googlenet多输出一样。

--------

18然后多跳推理的数据集，名字很有意思 hotpotQA火锅QA

看到他的json结构是每个问题对应的答案段落有很多。

-----

19 作者做了一些消融实验 和 和其他人对比的实验

基本上都领先，只有在一般疑问句的效果不如第二名。

这里我怀疑是CLS提取的语义经过研究是很弱的甚至不如word2vec、glove。这是2020年字节一篇文章研究发现的，提出了一个flow模型矫正了bert

回去讲算法流程

----

最后展望

1234

认知心理学家将知识分为两类:自下而上的信息，是直接来自我们感官的信息;还有自上而下的知识，是我们对世界的先验知识,例如,字母和数字是两个不同的类别，单词和数字是由来自这些类别之中的元素所组成的，等等。这种模棱两可的B/13图像，在不同的上下文中会呈现出不同的面貌，因为我们会尝试着将落在视网膜上的光线与合乎逻辑的世界相结合。

找到一种方法将自下而上和自上而下两者整合为一体，是人工智能的当务之急，却常常被人忽视。

